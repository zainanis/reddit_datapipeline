# Reddit Data Engineering Pipeline

An ETL pipeline that extracts data from Reddit, processes it through AWS services, and loads it into a data warehouse. Built to learn modern data engineering practices and cloud technologies.

## ğŸ“‹ Overview

This project implements an automated data pipeline that:

- Extracts Reddit posts and comments using Reddit's REST API
- Orchestrates workflows with Apache Airflow
- Stores raw data in Amazon S3
- Transforms data using AWS Glue and Athena
- Loads processed data into Redshift for analytics

**Note:** This implementation uses Reddit's direct API endpoints instead of the PRAW library since API tokens weren't available during development.

## ğŸ—ï¸ Architecture

```
![alt text](assets/image.png)
```

## ğŸ› ï¸ Tech Stack

- **Python 3.9+** - Core programming language
- **Apache Airflow** - Workflow orchestration
- **Celery** - Distributed task queue
- **PostgreSQL** - Airflow metadata storage
- **Docker & Docker Compose** - Containerization
- **AWS S3** - Data lake storage
- **AWS Glue** - Data catalog
- **Amazon Athena** - Query engine
- **Amazon Redshift** - Data warehouse
- **Reddit REST API** - Data source

## ğŸš€ Setup

### Prerequisites

- Docker Desktop
- Python 3.9+
- AWS Account
- Reddit Account

### Installation

1. **Clone the repository**

```bash
git clone https://github.com/zainanis/reddit_datapipeline.git
cd reddit_datapipeline
```

2. **Configure credentials**

```bash
cp config/example.config.conf config/config.conf
mkdir logs data/output
```

Edit `config/config.conf` with your credentials:

```ini

[aws]
aws_access_key_id = your_key
aws_secret_access_key = your_secret
aws_region = us-east-1
aws_bucket_name = your-bucket

```

3. **Set up AWS resources**

   - Create S3 bucket
   - Configure IAM user with S3, Glue, Athena, Redshift permissions
   - Create Redshift cluster (optional for testing)

4. **Start services**

```bash
docker-compose up -d --build
```

4. **Initialize Airflow**

```bash
docker-compose up airflow-init
```

7. **Access Airflow**
   - URL: http://localhost:8080
   - Username: `admin`
   - Password: `admin`

## ğŸ“ Project Structure

```
â”œâ”€â”€ dags/                  # Airflow DAG definitions
â”œâ”€â”€ pipelines/             # Data extraction logic
â”‚   â”œâ”€â”€ reddit_pipeline_v02.py
â”‚   â””â”€â”€ upload_s3_pipeline.py
â”œâ”€â”€ etls/                  # ETL transformations
â”‚   â”œâ”€â”€ reddit_etl.py
â”‚   â””â”€â”€ aws_etl.py
â”œâ”€â”€ config/                # Configuration files
â”œâ”€â”€ docker-compose.yml     # Docker services
â”œâ”€â”€ Dockerfile
â””â”€â”€ requirements.txt       # Python dependencies
```

## ğŸ”„ Pipeline Flow

1. **Extract**: Airflow triggers scheduled job to fetch Reddit data via API
2. **Store**: Raw JSON data uploaded to S3 bucket
3. **Catalog**: AWS Glue crawler scans and catalogs S3 data
4. **Transform**: Athena queries clean and structure the data
5. **Load**: Transformed data loaded into Redshift tables
6. **Automate**: Pipeline runs on defined schedule

## ğŸ¯ Key Features

- Automated data extraction on schedule
- Scalable cloud storage with S3
- Serverless data transformation
- Containerized deployment
- Monitoring via Airflow UI
- Modular and extensible codebase

## ğŸ“š What I Learned

- Building end-to-end ETL pipelines
- Workflow orchestration with Airflow
- AWS data services (S3, Glue, Athena, Redshift)
- Docker containerization
- REST API integration
- Data warehouse design

## ğŸ™ Acknowledgments

This project is a recreation of [airscholar/RedditDataEngineering](https://github.com/airscholar/RedditDataEngineering) for learning purposes.

## ğŸ“„ License

MIT License - Free to use for educational purposes.
